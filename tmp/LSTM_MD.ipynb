{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RihkyeTask + LSTM_MD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from task import RihkyeTask\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from temp_model import MD, LSTM_MD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cueingcontext = 2\n",
    "num_cue = 2\n",
    "num_rule = 2\n",
    "rule = [0, 1, 0, 1]\n",
    "blocklen = [500, 500, 200]\n",
    "block_cueingcontext = [0, 1, 0]\n",
    "tsteps = 200\n",
    "cuesteps = 100\n",
    "batch_size = 1 # always set to 1 right now\n",
    "\n",
    "dataset = RihkyeTask(num_cueingcontext=num_cueingcontext, num_cue=num_cue, num_rule=num_rule, rule=rule, blocklen=blocklen, \\\n",
    "block_cueingcontext=block_cueingcontext, tsteps=tsteps, cuesteps=cuesteps, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings\n",
    "input_size = 4 # 4 cues\n",
    "hidden_size = 200\n",
    "output_size = 2 # 2 rules\n",
    "num_layers = 1\n",
    "Num_MD = 10\n",
    "num_active = 5\n",
    "tsteps = 200\n",
    "\n",
    "model = LSTM_MD(input_size=input_size, hidden_size=hidden_size, output_size=output_size, num_layers=num_layers, Num_MD=Num_MD, \\\n",
    "    num_active=num_active, tsteps=tsteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print model paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_MD(\n",
      "  (lstm): LSTM(4, 200)\n",
      "  (fc): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "lstm.weight_ih_l0 torch.Size([800, 4])\n",
      "lstm.weight_hh_l0 torch.Size([800, 200])\n",
      "lstm.bias_ih_l0 torch.Size([800])\n",
      "lstm.bias_hh_l0 torch.Size([800])\n",
      "fc.weight torch.Size([2, 200])\n",
      "fc.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "for param in model.named_parameters():\n",
    "    print(param[0], param[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm.weight_ih_l0\n",
      "lstm.weight_hh_l0\n",
      "lstm.bias_ih_l0\n",
      "lstm.bias_hh_l0\n",
      "fc.weight\n",
      "fc.bias\n",
      "Total step: 1200\n",
      "Training sample index: 0-10\n",
      "loss: 0.17567\n",
      "Predicted left training time: 361 s\n",
      "\n",
      "Total step: 1200\n",
      "Training sample index: 10-20\n",
      "loss: 0.12678\n",
      "Predicted left training time: 368 s\n",
      "\n",
      "Total step: 1200\n",
      "Training sample index: 20-30\n",
      "loss: 0.11936\n",
      "Predicted left training time: 354 s\n",
      "\n",
      "Total step: 1200\n",
      "Training sample index: 30-40\n",
      "loss: 0.11628\n",
      "Predicted left training time: 342 s\n",
      "\n",
      "Total step: 1200\n",
      "Training sample index: 40-50\n",
      "loss: 0.10697\n",
      "Predicted left training time: 343 s\n",
      "\n",
      "Total step: 1200\n",
      "Training sample index: 50-60\n",
      "loss: 0.04710\n",
      "Predicted left training time: 336 s\n",
      "\n",
      "Total step: 1200\n",
      "Training sample index: 60-70\n",
      "loss: 0.03182\n",
      "Predicted left training time: 341 s\n",
      "\n",
      "Total step: 1200\n",
      "Training sample index: 70-80\n",
      "loss: 0.02410\n",
      "Predicted left training time: 332 s\n",
      "\n",
      "Total step: 1200\n",
      "Training sample index: 80-90\n",
      "loss: 0.02276\n",
      "Predicted left training time: 334 s\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-dd611a593816>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "training_params = list()\n",
    "for name, param in model.named_parameters():\n",
    "#     if 'lstm' not in name:\n",
    "    if True:\n",
    "        print(name)\n",
    "        training_params.append(param)\n",
    "optimizer = torch.optim.Adam(training_params, lr=1e-3)\n",
    "\n",
    "\n",
    "total_step = sum(blocklen)//batch_size\n",
    "print_step = 10\n",
    "running_loss = 0.0\n",
    "running_train_time = 0\n",
    "losses = []\n",
    "timestamps = []\n",
    "model_name = 'model-' + str(int(time.time()))\n",
    "savemodel = False\n",
    "\n",
    "\n",
    "for i in range(total_step):\n",
    "\n",
    "    train_time_start = time.time()\n",
    "\n",
    "    # extract data\n",
    "    inputs, labels = dataset()\n",
    "    inputs = torch.from_numpy(inputs).type(torch.float)\n",
    "    labels = torch.from_numpy(labels).type(torch.float)\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = model(inputs, labels)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # normalization\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    running_train_time += time.time() - train_time_start\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    if i % print_step == (print_step - 1):\n",
    "        print('Total step: {:d}'.format(total_step))\n",
    "        print('Training sample index: {:d}-{:d}'.format(i+1-print_step, i+1))\n",
    "\n",
    "        # running loss\n",
    "        print('loss: {:0.5f}'.format(running_loss / print_step))\n",
    "        losses.append(running_loss / print_step)\n",
    "        timestamps.append(i+1-print_step)\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # training time\n",
    "        print('Predicted left training time: {:0.0f} s'.format(\n",
    "        (running_train_time) * (total_step - i - 1) / print_step),\n",
    "        end='\\n\\n')\n",
    "        running_train_time = 0\n",
    "\n",
    "        if savemodel:\n",
    "            # save model every print_step\n",
    "            fname = os.path.join('models', model_name + '.pt')\n",
    "            torch.save(model.state_dict(), fname)\n",
    "\n",
    "            # save info of the model\n",
    "            fpath = os.path.join('models', model_name + '.txt')\n",
    "            with open(fpath, 'w') as f:\n",
    "                f.write('input_size = ' + str(input_size) + '\\n')\n",
    "                f.write('hidden_size = ' + str(hidden_size) + '\\n')\n",
    "                f.write('output_size = ' + str(output_size) + '\\n')\n",
    "                f.write('num_layers = ' + str(num_layers) + '\\n')\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(timestamps, losses, label='Without MD')\n",
    "plt.xlabel('Cycles')\n",
    "plt.ylabel('MSE loss')\n",
    "plt.legend()\n",
    "plt.xticks([0, 500, 1000, 1200])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deprecated\n",
    "#lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=1, proj_size=2)\n",
    "lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=1)\n",
    "input = torch.randn(5, 3, 10)\n",
    "#h0 = torch.randn(1, 3, 20)\n",
    "#c0 = torch.randn(1, 3, 20)\n",
    "output, _= lstm(input)\n",
    "\n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
