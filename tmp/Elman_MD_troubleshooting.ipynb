{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('deeplearning': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8d1b0cb2583ed9fdcdbc8ee19dfc4bbbc916785483064bd7ff39d2deaff2ffb4"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import imageio\n",
    "from pygifsicle import optimize\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "root = os.getcwd()\n",
    "sys.path.append(root)\n",
    "sys.path.append('..')\n",
    "\n",
    "from task import RikhyeTaskBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------- Rikhye dataset with batch dimension ----------------#\n",
    "\n",
    "# set random seed\n",
    "RNGSEED = 5\n",
    "np.random.seed([RNGSEED])\n",
    "torch.manual_seed(RNGSEED) \n",
    "os.environ['PYTHONHASHSEED'] = str(RNGSEED)\n",
    "\n",
    "num_cueingcontext = 2\n",
    "num_cue = 2\n",
    "num_rule = 2\n",
    "rule = [0, 1, 0, 1]\n",
    "blocklen = [200, 200, 100]\n",
    "block_cueingcontext = [0, 1, 0]\n",
    "tsteps = 200\n",
    "cuesteps = 100\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "# create a dataset\n",
    "dataset = RikhyeTaskBatch(num_cueingcontext=num_cueingcontext, num_cue=num_cue, num_rule=num_rule,\\\n",
    "                          rule=rule, blocklen=blocklen, block_cueingcontext=block_cueingcontext,\\\n",
    "                          tsteps=tsteps, cuesteps=cuesteps, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Elman(nn.Module):\n",
    "    \"\"\"Elman RNN that can take in MD inputs.\n",
    "    Args:\n",
    "        input_size: Number of input neurons\n",
    "        hidden_size: Number of hidden neurons\n",
    "    Inputs:\n",
    "        input: (seq_len, batch, input_size), external input; seq_len is set to 1\n",
    "        hidden: (batch, hidden_size), initial hidden activity;\n",
    "        mdinput: (batch, hidden_size), MD input;\n",
    "\n",
    "    Acknowlegement:\n",
    "        based on Robert Yang's CTRNN class\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, nonlinearity='tanh'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        if nonlinearity == 'relu':\n",
    "            self.activation = torch.relu\n",
    "        else:\n",
    "            self.activation = torch.tanh\n",
    "\n",
    "        # Sensory input -> RNN\n",
    "        self.input2h = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        # RNN -> RNN\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        k = (1./self.hidden_size)**0.5\n",
    "        nn.init.uniform_(self.h2h.weight, a=-k, b=k) # same as pytorch built-in RNN module\n",
    "\n",
    "        \n",
    "    def init_hidden(self, input):\n",
    "        batch_size = input.shape[1]\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden=None, mdinput=None):\n",
    "        '''\n",
    "        Propogate input through the network\n",
    "        '''\n",
    "        # TODO: input.shape has to be [timestep=1, batch_size, input_size]\n",
    "        \n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(input)\n",
    "\n",
    "        pre_activation = self.input2h(input) + self.h2h(hidden)\n",
    "\n",
    "        if mdinput is not None:\n",
    "            pre_activation += mdinput\n",
    "        \n",
    "        hidden = self.activation(pre_activation)\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "\n",
    "class Elman_MD(nn.Module):\n",
    "    \"\"\"Elman RNN with a MD layer\n",
    "    Parameters:\n",
    "    input_size: int, RNN input size\n",
    "    hidden_size: int, RNN hidden size\n",
    "    output_size: int, output layer size\n",
    "    num_layers: int, number of RNN layers\n",
    "    nonlinearity: str, 'tanh' or 'relu', nonlinearity in RNN layers\n",
    "    Num_MD: int, number of neurons in MD layer\n",
    "    num_active: int, number of active neurons in MD layer (refer to top K winner-take-all)\n",
    "    tsteps: int, length of a trial, equals to cuesteps + delaysteps\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, nonlinearity, Num_MD, num_active, tsteps, MDeffect=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.tsteps = tsteps\n",
    "\n",
    "        # PFC layer / Elman RNN layer\n",
    "        self.rnn = Elman(input_size, hidden_size, nonlinearity)\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        '''\n",
    "        Propogate input through the network\n",
    "        '''\n",
    "        n_time = input.shape[0]\n",
    "        batch_size = input.shape[1]\n",
    "\n",
    "        # initialize variables for saving important network activities\n",
    "        RNN_output = torch.zeros((n_time, batch_size, self.hidden_size))\n",
    "\n",
    "        # initialize RNN and MD activities\n",
    "        RNN_hidden_t = torch.zeros((1, batch_size, self.hidden_size))\n",
    "        \n",
    "\n",
    "        for t in range(n_time):\n",
    "            input_t = input[t, ...].unsqueeze(dim=0)\n",
    "            RNN_hidden_t = self.rnn(input_t, RNN_hidden_t)\n",
    "            RNN_output[t, :, :] = RNN_hidden_t\n",
    "\n",
    "        model_out = torch.tanh(self.fc(RNN_output))\n",
    "\n",
    "        return model_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "rnn.input2h.weight\n",
      "rnn.input2h.bias\n",
      "rnn.h2h.weight\n",
      "rnn.h2h.bias\n",
      "fc.weight\n",
      "fc.bias\n",
      "tensor([0.4834, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4834, 0.5936, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4834, 0.5936, 0.5341, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4834, 0.5936, 0.5341, 0.4773, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4834, 0.5936, 0.5341, 0.4773, 0.4647, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4834, 0.5936, 0.5341, 0.4773, 0.4647, 0.4759, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4834, 0.5936, 0.5341, 0.4773, 0.4647, 0.4759, 0.4805, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4834, 0.5936, 0.5341, 0.4773, 0.4647, 0.4759, 0.4805, 0.4836, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4834, 0.5936, 0.5341, 0.4773, 0.4647, 0.4759, 0.4805, 0.4836, 0.4823,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4834, 0.5936, 0.5341, 0.4773, 0.4647, 0.4759, 0.4805, 0.4836, 0.4823,\n",
      "        0.4825], grad_fn=<SelectBackward>)\n",
      "tensor([0.4834, 0.5936, 0.5341, 0.4773, 0.4647, 0.4759, 0.4805, 0.4836, 0.4823,\n",
      "        0.4825], grad_fn=<SelectBackward>)\n",
      "tensor([0.4826, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4826, 0.5301, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4826, 0.5301, 0.4498, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4826, 0.5301, 0.4498, 0.4080, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4826, 0.5301, 0.4498, 0.4080, 0.3960, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4826, 0.5301, 0.4498, 0.4080, 0.3960, 0.4206, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4826, 0.5301, 0.4498, 0.4080, 0.3960, 0.4206, 0.4320, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4826, 0.5301, 0.4498, 0.4080, 0.3960, 0.4206, 0.4320, 0.4331, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4826, 0.5301, 0.4498, 0.4080, 0.3960, 0.4206, 0.4320, 0.4331, 0.4327,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4826, 0.5301, 0.4498, 0.4080, 0.3960, 0.4206, 0.4320, 0.4331, 0.4327,\n",
      "        0.4338], grad_fn=<SelectBackward>)\n",
      "tensor([0.4826, 0.5301, 0.4498, 0.4080, 0.3960, 0.4206, 0.4320, 0.4331, 0.4327,\n",
      "        0.4338], grad_fn=<SelectBackward>)\n",
      "tensor([0.4820, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4820, 0.4841, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4820, 0.4841, 0.3538, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4820, 0.4841, 0.3538, 0.3349, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4820, 0.4841, 0.3538, 0.3349, 0.3597, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4820, 0.4841, 0.3538, 0.3349, 0.3597, 0.4150, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4820, 0.4841, 0.3538, 0.3349, 0.3597, 0.4150, 0.4346, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4820, 0.4841, 0.3538, 0.3349, 0.3597, 0.4150, 0.4346, 0.4371, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4820, 0.4841, 0.3538, 0.3349, 0.3597, 0.4150, 0.4346, 0.4371, 0.4360,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4820, 0.4841, 0.3538, 0.3349, 0.3597, 0.4150, 0.4346, 0.4371, 0.4360,\n",
      "        0.4380], grad_fn=<SelectBackward>)\n",
      "tensor([0.4820, 0.4841, 0.3538, 0.3349, 0.3597, 0.4150, 0.4346, 0.4371, 0.4360,\n",
      "        0.4380], grad_fn=<SelectBackward>)\n",
      "tensor([-0.1809,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([-0.1809, -0.5699,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([-0.1809, -0.5699, -0.7970,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([-0.1809, -0.5699, -0.7970, -0.8547,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([-0.1809, -0.5699, -0.7970, -0.8547, -0.8616,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([-0.1809, -0.5699, -0.7970, -0.8547, -0.8616, -0.8762,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([-0.1809, -0.5699, -0.7970, -0.8547, -0.8616, -0.8762, -0.8851,  0.0000,\n",
      "         0.0000,  0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([-0.1809, -0.5699, -0.7970, -0.8547, -0.8616, -0.8762, -0.8851, -0.8885,\n",
      "         0.0000,  0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([-0.1809, -0.5699, -0.7970, -0.8547, -0.8616, -0.8762, -0.8851, -0.8885,\n",
      "        -0.8891,  0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([-0.1809, -0.5699, -0.7970, -0.8547, -0.8616, -0.8762, -0.8851, -0.8885,\n",
      "        -0.8891, -0.8892], grad_fn=<SelectBackward>)\n",
      "tensor([-0.1809, -0.5699, -0.7970, -0.8547, -0.8616, -0.8762, -0.8851, -0.8885,\n",
      "        -0.8891, -0.8892], grad_fn=<SelectBackward>)\n",
      "tensor([0.4838, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4838, 0.6020, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4838, 0.6020, 0.7163, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4838, 0.6020, 0.7163, 0.8978, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4838, 0.6020, 0.7163, 0.8978, 0.9550, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4838, 0.6020, 0.7163, 0.8978, 0.9550, 0.9659, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4838, 0.6020, 0.7163, 0.8978, 0.9550, 0.9659, 0.9671, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4838, 0.6020, 0.7163, 0.8978, 0.9550, 0.9659, 0.9671, 0.9682, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4838, 0.6020, 0.7163, 0.8978, 0.9550, 0.9659, 0.9671, 0.9682, 0.9689,\n",
      "        0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([0.4838, 0.6020, 0.7163, 0.8978, 0.9550, 0.9659, 0.9671, 0.9682, 0.9689,\n",
      "        0.9690], grad_fn=<SelectBackward>)\n",
      "tensor([0.4838, 0.6020, 0.7163, 0.8978, 0.9550, 0.9659, 0.9671, 0.9682, 0.9689,\n",
      "        0.9690], grad_fn=<SelectBackward>)\n",
      "tensor([-0.1821,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([-0.1821, -0.6509,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([-0.1821, -0.6509, -0.9038,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([-0.1821, -0.6509, -0.9038, -0.9590,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([-0.1821, -0.6509, -0.9038, -0.9590, -0.9782,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([-0.1821, -0.6509, -0.9038, -0.9590, -0.9782, -0.9866,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([-0.1821, -0.6509, -0.9038, -0.9590, -0.9782, -0.9866, -0.9880,  0.0000,\n",
      "         0.0000,  0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([-0.1821, -0.6509, -0.9038, -0.9590, -0.9782, -0.9866, -0.9880, -0.9881,\n",
      "         0.0000,  0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([-0.1821, -0.6509, -0.9038, -0.9590, -0.9782, -0.9866, -0.9880, -0.9881,\n",
      "        -0.9882,  0.0000], grad_fn=<SelectBackward>)\n",
      "tensor([-0.1821, -0.6509, -0.9038, -0.9590, -0.9782, -0.9866, -0.9880, -0.9881,\n",
      "        -0.9882, -0.9883], grad_fn=<SelectBackward>)\n",
      "tensor([-0.1821, -0.6509, -0.9038, -0.9590, -0.9782, -0.9866, -0.9880, -0.9881,\n",
      "        -0.9882, -0.9883], grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-749d94d62123>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;31m###print(model.parm['rnn.input2h.weight'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;31m###print(model.parm['rnn.h2h.weight'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# clip gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\Anaconda_setting\\envs\\deeplearning\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\Anaconda_setting\\envs\\deeplearning\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_size = 4          # 4 cues\n",
    "hidden_size = 1000      # number of PFC neurons\n",
    "output_size = 2         # 2 rules\n",
    "num_layers = 1\n",
    "nonlinearity = 'tanh'\n",
    "Num_MD = 10\n",
    "num_active = 5\n",
    "MDeffect = False\n",
    "\n",
    "# create model\n",
    "model = Elman_MD(input_size=input_size, hidden_size=hidden_size, output_size=output_size,\\\n",
    "                 num_layers=num_layers, nonlinearity=nonlinearity, Num_MD=Num_MD, num_active=num_active,\\\n",
    "                 tsteps=tsteps, MDeffect=MDeffect)\n",
    "\n",
    "for name, parm in model.named_parameters():\n",
    "    print(name)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "total_step = sum(blocklen)//batch_size\n",
    "print_step = 10 # print statistics every print_step\n",
    "running_loss = 0.0\n",
    "running_mseloss = 0.0\n",
    "running_train_time = 0\n",
    "\n",
    "\n",
    "for i in range(total_step):\n",
    "\n",
    "    train_time_start = time.time()\n",
    "\n",
    "    # extract data\n",
    "    inputs, labels = dataset()\n",
    "    inputs = torch.from_numpy(inputs).type(torch.float)\n",
    "    labels = torch.from_numpy(labels).type(torch.float)\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward\n",
    "    outputs = model(inputs, labels)\n",
    "    ###print('MSE', criterion(outputs, labels))\n",
    "    ###print('reg', disjoint_penalty(model, reg=reg))\n",
    "\n",
    "    # backward + optimize\n",
    "    loss = criterion(outputs, labels)\n",
    "    ###print(loss)\n",
    "    ###print(model.parm['rnn.input2h.weight'])\n",
    "    ###print(model.parm['rnn.h2h.weight'])\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # clip gradients\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    # print statistics\n",
    "    running_train_time += time.time() - train_time_start\n",
    "    running_loss += loss.item()\n",
    "    running_mseloss += loss.item()\n",
    "\n",
    "    if i % print_step == (print_step - 1):\n",
    "\n",
    "        print('Total step: {:d}'.format(total_step))\n",
    "        print('Training sample index: {:d}-{:d}'.format(i+1-print_step, i+1))\n",
    "\n",
    "        # running loss\n",
    "        print('Total loss: {:0.5f};'.format(running_loss / print_step), 'MSE loss: {:0.5f}'.format(running_mseloss / print_step))\n",
    "        running_loss = 0.0\n",
    "        running_mseloss = 0.0\n",
    "\n",
    "        # training time\n",
    "        print('Predicted left training time: {:0.0f} s'.format(\n",
    "        (running_train_time) * (total_step - i - 1) / print_step),\n",
    "        end='\\n\\n')\n",
    "        running_train_time = 0\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ]
}